{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r'C:\\Users\\okechukwu chude\\Documents\\NLP\\text extraction\\bert-crf\\#project files\\training.csv')\n",
    "test_data = pd.read_csv(r'C:\\Users\\okechukwu chude\\Documents\\NLP\\text extraction\\bert-crf\\#project files\\testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6748, 1688)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ **2167-7-7 ** ] 1:18 AM CHEST ( SINGLE VIEW ...</td>\n",
       "      <td>O,O,O,O,O,O,B,I,I,I,I,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Overall left ventricular systolic function is ...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>( every 6 hours ) as needed for Pain / HA : Do...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Successful embolization of a right hepatic art...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,B,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INDICATION : 23 -year-old female with traumati...</td>\n",
       "      <td>O,O,O,O,O,O,B,I,I,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [ **2167-7-7 ** ] 1:18 AM CHEST ( SINGLE VIEW ...   \n",
       "1  Overall left ventricular systolic function is ...   \n",
       "2  ( every 6 hours ) as needed for Pain / HA : Do...   \n",
       "3  Successful embolization of a right hepatic art...   \n",
       "4  INDICATION : 23 -year-old female with traumati...   \n",
       "\n",
       "                                         word_labels  \n",
       "0  O,O,O,O,O,O,B,I,I,I,I,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
       "1                    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O  \n",
       "2              O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O  \n",
       "3                  O,O,O,O,O,O,O,O,B,O,O,O,O,O,O,O,O  \n",
       "4                                O,O,O,O,O,O,B,I,I,O  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Overall left ventricular systolic function is low normal ( LVEF 50 - 55 % ) .'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[1].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[1].word_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id: {'O': 0, 'B': 1, 'I': 2}\n",
      "id2label: {0: 'O', 1: 'B', 2: 'I'}\n"
     ]
    }
   ],
   "source": [
    "# Function to generate the dictionaries\n",
    "def generate_label_dicts(train_data):\n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "    label_counts = {}\n",
    "    \n",
    "    for labels in train_data['word_labels']:\n",
    "        for label in labels.split(','):\n",
    "            if label not in label_counts:\n",
    "                label_counts[label] = len(label_counts)\n",
    "                label2id[label] = label_counts[label]\n",
    "                id2label[label_counts[label]] = label\n",
    "    \n",
    "    return label2id, id2label\n",
    "\n",
    "# Generate dictionaries\n",
    "label2id, id2label = generate_label_dicts(train_data)\n",
    "\n",
    "print(\"label2id:\", label2id)\n",
    "print(\"id2label:\", id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.19k/1.19k [00:00<?, ?B/s]\n",
      "C:\\Users\\okechukwu chude\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\okechukwu chude\\.cache\\huggingface\\hub\\models--praneethvasarla--med-bert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.56MB/s]\n",
      "tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 1.02MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"praneethvasarla/med-bert\")\n",
    "MAX_LEN = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.sentence[index]\n",
    "        word_labels = self.data.word_labels[index]\n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "\n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "\n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        \n",
    "\n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (6748, 2)\n",
      "TEST Dataset: (1688, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = dataset(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1031,  1008,  1008, 20294,  2581,  1011,  1021,  1011,  1021,\n",
       "         1008,  1008,  1033,  1015,  1024,  2324,  2572,  3108,  1006,  2309,\n",
       "         3193,  1007, 12528,  1001,  1031,  1008,  1008, 12528,  2193,  1006,\n",
       "         2557,  6483,  1007,  2753, 22907,  2629,  1008,  1008,  1033,  3114,\n",
       "         1024,  1054,  1013,  1051,  1052,  2532,  1035,  1035,  1035,  1035,\n",
       "         1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,\n",
       "         1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,\n",
       "         1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,\n",
       "         1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,\n",
       "         1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,\n",
       "         1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,\n",
       "         1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,  1035,\n",
       "         1035,  1035,  1035,  1035,   102,     0,     0,     0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0][\"ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       O\n",
      "[           O\n",
      "*           O\n",
      "*           O\n",
      "216         O\n",
      "##7         O\n",
      "-           O\n",
      "7           O\n",
      "-           O\n",
      "7           O\n",
      "*           O\n",
      "*           O\n",
      "]           O\n",
      "1           O\n",
      ":           O\n",
      "18          O\n",
      "am          O\n",
      "chest       B\n",
      "(           I\n",
      "single      I\n",
      "view        I\n",
      ")           I\n",
      "clip        O\n",
      "#           O\n",
      "[           O\n",
      "*           O\n",
      "*           O\n",
      "clip        O\n",
      "number      O\n",
      "(           O\n"
     ]
    }
   ],
   "source": [
    "# print the first 30 tokens and corresponding labels\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"ids\"][:30]), training_set[0][\"targets\"][:30]):\n",
    "  print('{0:10}  {1}'.format(token, id2label[label.item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Generate labeled_texts.jsonl\n",
    "labeled_texts = []\n",
    "\n",
    "for index in range(len(training_set)):\n",
    "    sample = training_set[index]\n",
    "    sample_dict = {\n",
    "        \"id\": index,\n",
    "        \"input_ids\": sample[\"ids\"].tolist(),\n",
    "        \"text_labels\": [id2label[label.item()] for label in sample[\"targets\"]],\n",
    "        \"labels\": sample[\"targets\"].tolist()\n",
    "    }\n",
    "    labeled_texts.append(sample_dict)\n",
    "\n",
    "with open(\"med-bert_labeled_texts.jsonl\", \"w\") as outfile:\n",
    "    for sample in labeled_texts:\n",
    "        json.dump(sample, outfile)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "# Generate label2id.jsonl\n",
    "with open(\"med-bert_label2id.jsonl\", \"w\") as outfile:\n",
    "    json.dump(label2id, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "using_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1a69bf678df178a0a83c1bc249e551d0589a35527ff82fba5716df60535a642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
